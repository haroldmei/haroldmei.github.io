---
layout: post
title: "This is a test"
categories: misc
---


## Statistical Decision Theory

There are many point estimators, such as
1. MLE
2. MM
3. Posterior Mean

And there are more. The question here is, which one should we choose and why? Decision theory can answer this question.  

Consider the true unknown parameter $\theta$ and its parameter space $\Theta$; it's estimator is $\hat{\theta}$. An estimator is called a $\textbf{decision rule}$, and fitted values of a decision rule are called $\textbf{actions}.

Loss function is the measurement of descrepency between $\theta$ and $\hat{\theta}$, denoted as $L(\theta,\hat{\theta})$. Examples of loss functions are:
1. squared error loss: $L(\theta, \hat{\theta})=(\theta-\hat{\theta})^2$
2. absolute error loss: $L(\theta, \hat{\theta})=|\theta-\hat{\theta}|$
3. $L_p$ loss
4. zero-one loss: $1{\theta=\hat{\theta}}$
5. KL loss: $L(\theta, \hat{\theta})=KL(\theta, \hat{\theta})$

#### 12.1 Definition: Risk
The risk is the expected loss over data
$$
R(\theta, \hat{\theta})=\mathbb{E}_{\theta}(L(\theta, \hat{\theta}))=\int L(\theta, \hat{\theta}) f(x|\theta)dx
$$
The estimator $\hat{\theta}$ is a statistic, also a function of the data.

The risk of a squared error loss is called Mean Squred Error, AKA MSE.
$$
\begin{eqnarray*}
R(\theta, \hat{\theta})&=&\mathbb{E}_{\theta}(\theta-\hat{\theta})^2 \\
&=& MSE \\
&=& \mathbb{V}_{\hat{\theta}}(\theta)+bias_{\theta}^2(\hat{\theta})
\end{eqnarray*}
$$

### 12.6 Admissibility
Minimax estimators and Bayes estimators are good estimators in the sense that they both have small risks.  
It is also useful to characterize bad estimators.  

#### 12.17 Definition
An estimator $\hat{\theta}$ is said to be inadmissible if there exists another estimator $\hat{\theta}'$ such that
$$
R(\theta,\hat{\theta}') \le R(\theta,\hat{\theta}) \text{ for all } \theta \text{ and } \\
R(\theta,\hat{\theta}') \lt R(\theta,\hat{\theta}) \text{ for at least one } \theta .
$$
Otherwise, $\theta$ is addmissible.


#### 12.18 Example. 
Let $X \sim N(\theta,1)$ and consider estimating $\theta$ with squared error loss. Let $\hat{\theta}=3$. Suppose it is inadmissible, then there exists another estimator $\hat{\theta}^'$ with a smaller risk for all $\theta$. Especially $R(3,\hat{\theta}') < R(3,\hat{\theta}) =0$. Hence $0=R(3,\hat{\theta}')=\int (\hat{\theta}'-3)f(x;3)dx$, thus $\hat{\theta}'=3=\hat{\theta}$ which means $\hat{\theta}$ is admissible.  
An estimator is admissible doesn't mean it is a good decision rule.  

#### 12.19 Theorem (Bayes rules are admissible)
Suppose $\Theta \subset \mathbb{R}$ and $R(\theta,\hat{\theta})$ is a continuous function of $\theta$ for every $\hat\theta$. Let f be a prior density with full support, meaning $\int_{\theta-\epsilon}^{\theta+\epsilon}f(\theta)d\theta >0$ for every $\theta$ and $\epsilon$. Let $\hat{\theta}^f$ be a Beyes rule, if the Bayes risk is finite then $\hat{\theta}^f$ is admissible.  

PROOF



#### 12.20 Theorem
Let $X_1,X_2,...,X_n \sim N(\mu,\delta^2)$. Under squared error loss, $\overline{X}$ is admissible.  
The idea is, the posterior mean is admissible for any strictly positive prior. Take the prior $N(a,b^2)$, when $b^2$ is very large, the posterior mean is approximately equal to $\overline X$ which is admissible.   

PROOF

#### 12.21 Theorem
Suppose $\hat \theta$ has constant risk and is admissible, then it is minimax.  

PROOF

#### 12.22 Theorem
Let $X_1,X_2,...,X_n \sim N(\theta,1)$, Then under squared error loss, $\hat \theta = \overline X$ is minimax.  

PROOF

Although minimax rules are not guarranteed to be admissible, they are close to admissible.  
$\textbf{Strongly inadmissible}$: at every point $\theta$ there exists a $\hat{\theta}'$ that $R(\theta,\hat{\theta}')<R(\theta,\hat{\theta})$  

#### 12.23 Theorem
If $\hat \theta$ is minimax, then it is not strongly inadmissible.

### Stein's paradox
Suppose $X \sim N(\theta,1)$ and consider estimating $\theta$ with squared error loss. From 12.18 we know that $\hat{\theta}=X$ is admissible.  
Now consider estimating two unrelated normal $X_1 \sim N(\theta_1,1)$ and $X_2 \sim N(\theta_2,1)$ with loss $L(\theta,\hat{\theta})=\sum_{i=1}^2 (\theta_i - \hat{\theta}_i)^2$, not supprisingly this is still admissible.  
Now consider generalization to k unrelated normals, let $\theta = (\theta_1,...,\theta_k)$ and $X=(X_1,...,X_k)$ and loss $L(\theta,\hat{\theta})=\sum_{i=1}^k (\theta_i - \hat{\theta}_i)^2$. Astoundingly when $k \ge 3$, the estimator $X$ is inadmissible. It can be shown that James-Stain's estimator $\hat{\theta}^S$ has a smaller risk, where 
$$
\hat{\theta}_i^S = \left(1 - \frac{k-2}{\sum_i X_i^2} \right)^+ X_i
$$
and $z^+ = max(z,0)$.  
This estimator shrinks the $X_i$'s towards 0. 

The message is that when estimating many parameters, there is great value in shrinking the estimates.


```python

```
